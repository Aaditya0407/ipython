{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch14 Working with CSV Files and JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with CSV Files and JSON Data\n",
    "\n",
    "- PDF와 Word는 Binary format 이었지만 CSV와 JSON 파일은 plain text다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5/2014 13:34,Apples,73\r",
      "\r\n",
      "4/5/2014 3:41,Cherries,85\r",
      "\r\n",
      "4/6/2014 12:46,Pears,14\r",
      "\r\n",
      "4/8/2014 8:59,Oranges,52\r",
      "\r\n",
      "4/10/2014 2:07,Apples,152\r",
      "\r\n",
      "4/10/2014 18:10,Bananas,23\r",
      "\r\n",
      "4/10/2014 2:40,Strawberries,98\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat src/example.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV features\n",
    "\n",
    "- Don't have types for their values - everything is a string\n",
    "- Don't have settings for font size or color\n",
    "- Don't have multiple worksheets\n",
    "- Can't specify cell widths and heights\n",
    "- Can't have merged cells\n",
    "- Can't have images or charts embedded in them\n",
    "\n",
    "\n",
    "### CSV advantages\n",
    "\n",
    "- simplicity\n",
    "- 많은 프로그램에서 csv 파일을 지원한다.\n",
    "- split() 로만 하게 되면 잠재적인 escape character를 처리할 수 없다.\n",
    "- 너는 항상 csv 모듈을 써서 읽고 써라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('src/example.csv') as exampleFile:\n",
    "    exampleReader = csv.reader(exampleFile)\n",
    "    exampleData = list(exampleReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4/5/2014 13:34', 'Apples', '73'],\n",
       " ['4/5/2014 3:41', 'Cherries', '85'],\n",
       " ['4/6/2014 12:46', 'Pears', '14'],\n",
       " ['4/8/2014 8:59', 'Oranges', '52'],\n",
       " ['4/10/2014 2:07', 'Apples', '152'],\n",
       " ['4/10/2014 18:10', 'Bananas', '23'],\n",
       " ['4/10/2014 2:40', 'Strawberries', '98']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x10951e1a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_csv.reader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-51372de267ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexampleReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '_csv.reader' object is not callable"
     ]
    }
   ],
   "source": [
    "exampleReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4/5/2014 13:34'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09aeab80adee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexampleData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "exampleData[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cherries'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strawberries'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData[6][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from Reader Objects in a for Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row # 1 ['4/5/2014 13:34', 'Apples', '73']\n",
      "Row # 2 ['4/5/2014 3:41', 'Cherries', '85']\n",
      "Row # 3 ['4/6/2014 12:46', 'Pears', '14']\n",
      "Row # 4 ['4/8/2014 8:59', 'Oranges', '52']\n",
      "Row # 5 ['4/10/2014 2:07', 'Apples', '152']\n",
      "Row # 6 ['4/10/2014 18:10', 'Bananas', '23']\n",
      "Row # 7 ['4/10/2014 2:40', 'Strawberries', '98']\n"
     ]
    }
   ],
   "source": [
    "with open('src/example.csv') as exampleFile:\n",
    "    exampleReader = csv.reader(exampleFile)\n",
    "    for row in exampleReader:\n",
    "        print('Row # {} {}'.format(exampleReader.line_num, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer Objects\n",
    "\n",
    "- [2. Built\\-in Functions — Python 2.7.10 documentation](https://docs.python.org/2/library/functions.html#open): newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = open('output.csv', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWriter = csv.writer(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWriter.writerow(['spam', 'eggs', 'bacon', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWriter.writerow(['Hello, world!', 'eggs', 'bacon', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputWriter.writerow([1, 2, 3, 3.141592, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam,eggs,bacon,ham\r",
      "\r\n",
      "\"Hello, world!\",eggs,bacon,ham\r",
      "\r\n",
      "1,2,3,3.141592,4\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# ,가 있어서 \"\"로 escape 시켜줌\n",
    "!cat output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The delimiter and lineterminator Keyword Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvFile = open('example.tsv', 'w')\n",
    "csvWriter = csv.writer(csvFile, delimiter='\\t', lineterminator='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvWriter.writerow(['apples', 'oranges', 'grapes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvWriter.writerow(['eggs', 'bacon', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvWriter.writerow(['spam', 'spam', 'spam', 'spam', 'spam', 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples\toranges\tgrapes\r\n",
      "\r\n",
      "eggs\tbacon\tham\r\n",
      "\r\n",
      "spam\tspam\tspam\tspam\tspam\tspam\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat example.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Removing the Header from CSV Files\n",
    "\n",
    "### High level Logic\n",
    "\n",
    "- Find all the CSV files in the current working directory.\n",
    "- Read in the full contents of each file.\n",
    "- Write out the contents, skipping the first line, to a new CSV file.\n",
    "\n",
    "### Code level Logic\n",
    "\n",
    "- Loop over a list of files from os.listdir(), skipping the non-CSV files.\n",
    "- Create a CSV reader object and read in the contents of the file, using the line_num attribute to figure out which line to skip.\n",
    "- Create a CSV Writer object and write out the read-in data to the new file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loop Through Each CSV File\n",
    "\n",
    "#### os.makedirs exist_ok\n",
    "\n",
    "- [mkdir \\-p functionality in python \\- Stack Overflow](http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python): 3.2 이상만 되네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "int(platform.python_version_tuple()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/headerRemoved'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname('/tmp/headerRemoved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dirname in module posixpath:\n",
      "\n",
      "dirname(p)\n",
      "    Returns the directory component of a pathname\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(os.path.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'census2010.py'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.splitext??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('census2010', '.py')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext(l[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get only text extension\n",
    "\n",
    "- [Extracting extension from filename in Python \\- Stack Overflow](http://stackoverflow.com/questions/541390/extracting-extension-from-filename-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.py'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('census2010.10.py')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'py'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('census2010.10.py')[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing removeCsvHeader/removecsvheader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile removeCsvHeader/removecsvheader.py\n",
    "# os.makedirs test purpose\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "\n",
    "def ensure_dir(dir_):\n",
    "    if not os.path.exists(dir_):\n",
    "        os.makedirs(dir_)\n",
    "\n",
    "python_version = int(platform.python_version_tuple()[0])\n",
    "if python_version == 3: \n",
    "    os.makedirs('headerRemoved', exist_ok=True)\n",
    "else:\n",
    "    ensure_dir('headerRemoved')\n",
    "\n",
    "# Looop through every file in the current working directory.\n",
    "for csvFilename in os.listdir('.'):\n",
    "    if not csvFilename.endswith('.csv'):\n",
    "        continue # skip non-csv files\n",
    "    print('Removing header from {}...'.format(csvFilename))\n",
    "    \n",
    "    # TODO: Read the CSV file in (skipping first row).\n",
    "    csvRows = []\n",
    "    with open(csvFilename) as csvFileObj:\n",
    "        readerObj = csv.reader(csvFileObj)\n",
    "        for row in readerObj:\n",
    "            if readerObj.line_num == 1:\n",
    "                continue\n",
    "            csvRows.append(row)\n",
    "    \n",
    "    # TODO: Write out the CSV file.\n",
    "    with open(os.path.join('headerRemoved', csvFilename), 'w') as csvFileObj:\n",
    "        csvWriter = csv.writer(csvFileObj)\n",
    "        for row in csvRows:\n",
    "            csvWriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/re4lfl0w/Documents/ipython/books/Automate_the_Boring_Sutff_with_Python/removeCsvHeader\n"
     ]
    }
   ],
   "source": [
    "%cd removeCsvHeader/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing header from NAICS_data_1048.csv...\n",
      "Removing header from NAICS_data_1218.csv...\n",
      "Removing header from NAICS_data_1657.csv...\n",
      "Removing header from NAICS_data_1751.csv...\n",
      "Removing header from NAICS_data_1814.csv...\n",
      "Removing header from NAICS_data_1817.csv...\n",
      "Removing header from NAICS_data_1889.csv...\n",
      "Removing header from NAICS_data_1952.csv...\n",
      "Removing header from NAICS_data_1973.csv...\n",
      "Removing header from NAICS_data_2066.csv...\n",
      "Removing header from NAICS_data_2092.csv...\n",
      "Removing header from NAICS_data_2183.csv...\n",
      "Removing header from NAICS_data_2346.csv...\n",
      "Removing header from NAICS_data_2427.csv...\n",
      "Removing header from NAICS_data_2648.csv...\n",
      "Removing header from NAICS_data_2799.csv...\n",
      "Removing header from NAICS_data_2828.csv...\n",
      "Removing header from NAICS_data_2959.csv...\n",
      "Removing header from NAICS_data_2988.csv...\n",
      "Removing header from NAICS_data_2993.csv...\n",
      "Removing header from NAICS_data_2994.csv...\n",
      "Removing header from NAICS_data_3044.csv...\n",
      "Removing header from NAICS_data_3073.csv...\n",
      "Removing header from NAICS_data_3075.csv...\n",
      "Removing header from NAICS_data_3144.csv...\n",
      "Removing header from NAICS_data_3197.csv...\n",
      "Removing header from NAICS_data_3237.csv...\n",
      "Removing header from NAICS_data_3494.csv...\n",
      "Removing header from NAICS_data_3495.csv...\n",
      "Removing header from NAICS_data_3731.csv...\n",
      "Removing header from NAICS_data_4031.csv...\n",
      "Removing header from NAICS_data_4125.csv...\n",
      "Removing header from NAICS_data_4213.csv...\n",
      "Removing header from NAICS_data_4215.csv...\n",
      "Removing header from NAICS_data_4329.csv...\n",
      "Removing header from NAICS_data_4436.csv...\n",
      "Removing header from NAICS_data_4525.csv...\n",
      "Removing header from NAICS_data_4610.csv...\n",
      "Removing header from NAICS_data_4618.csv...\n",
      "Removing header from NAICS_data_4699.csv...\n",
      "Removing header from NAICS_data_4896.csv...\n",
      "Removing header from NAICS_data_4938.csv...\n",
      "Removing header from NAICS_data_5060.csv...\n",
      "Removing header from NAICS_data_5092.csv...\n",
      "Removing header from NAICS_data_5305.csv...\n",
      "Removing header from NAICS_data_5341.csv...\n",
      "Removing header from NAICS_data_5364.csv...\n",
      "Removing header from NAICS_data_5631.csv...\n",
      "Removing header from NAICS_data_5890.csv...\n",
      "Removing header from NAICS_data_5899.csv...\n",
      "Removing header from NAICS_data_5992.csv...\n",
      "Removing header from NAICS_data_6161.csv...\n",
      "Removing header from NAICS_data_6181.csv...\n",
      "Removing header from NAICS_data_6329.csv...\n",
      "Removing header from NAICS_data_6335.csv...\n",
      "Removing header from NAICS_data_6397.csv...\n",
      "Removing header from NAICS_data_6493.csv...\n",
      "Removing header from NAICS_data_6637.csv...\n",
      "Removing header from NAICS_data_6700.csv...\n",
      "Removing header from NAICS_data_6842.csv...\n",
      "Removing header from NAICS_data_6904.csv...\n",
      "Removing header from NAICS_data_7028.csv...\n",
      "Removing header from NAICS_data_7102.csv...\n",
      "Removing header from NAICS_data_7138.csv...\n",
      "Removing header from NAICS_data_7226.csv...\n",
      "Removing header from NAICS_data_7338.csv...\n",
      "Removing header from NAICS_data_7383.csv...\n",
      "Removing header from NAICS_data_7388.csv...\n",
      "Removing header from NAICS_data_7427.csv...\n",
      "Removing header from NAICS_data_7535.csv...\n",
      "Removing header from NAICS_data_7642.csv...\n",
      "Removing header from NAICS_data_7677.csv...\n",
      "Removing header from NAICS_data_7765.csv...\n",
      "Removing header from NAICS_data_7830.csv...\n",
      "Removing header from NAICS_data_7833.csv...\n",
      "Removing header from NAICS_data_7845.csv...\n",
      "Removing header from NAICS_data_7913.csv...\n",
      "Removing header from NAICS_data_8015.csv...\n",
      "Removing header from NAICS_data_8085.csv...\n",
      "Removing header from NAICS_data_8131.csv...\n",
      "Removing header from NAICS_data_8196.csv...\n",
      "Removing header from NAICS_data_8397.csv...\n",
      "Removing header from NAICS_data_8403.csv...\n",
      "Removing header from NAICS_data_8499.csv...\n",
      "Removing header from NAICS_data_8522.csv...\n",
      "Removing header from NAICS_data_8545.csv...\n",
      "Removing header from NAICS_data_8700.csv...\n",
      "Removing header from NAICS_data_8749.csv...\n",
      "Removing header from NAICS_data_8760.csv...\n",
      "Removing header from NAICS_data_8832.csv...\n",
      "Removing header from NAICS_data_9012.csv...\n",
      "Removing header from NAICS_data_9066.csv...\n",
      "Removing header from NAICS_data_9103.csv...\n",
      "Removing header from NAICS_data_9139.csv...\n",
      "Removing header from NAICS_data_9165.csv...\n",
      "Removing header from NAICS_data_9250.csv...\n",
      "Removing header from NAICS_data_9448.csv...\n",
      "Removing header from NAICS_data_9825.csv...\n",
      "Removing header from NAICS_data_9834.csv...\n",
      "Removing header from NAICS_data_9986.csv...\n"
     ]
    }
   ],
   "source": [
    "!python removecsvheader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6408\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  26615  6  7 13:11 NAICS_data_1048.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24255  6  7 13:11 NAICS_data_1218.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  46908  6  7 13:11 NAICS_data_1657.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24707  6  7 13:11 NAICS_data_1751.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  20775  6  7 13:11 NAICS_data_1814.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24738  6  7 13:11 NAICS_data_1817.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  34045  6  7 13:11 NAICS_data_1889.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  35990  6  7 13:11 NAICS_data_1952.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  27629  6  7 13:11 NAICS_data_1973.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  35486  6  7 13:11 NAICS_data_2066.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  25764  6  7 13:11 NAICS_data_2092.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44995  6  7 13:11 NAICS_data_2183.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24911  6  7 13:11 NAICS_data_2346.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  39792  6  7 13:11 NAICS_data_2427.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  27458  6  7 13:11 NAICS_data_2648.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  16216  6  7 13:11 NAICS_data_2799.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  31589  6  7 13:11 NAICS_data_2828.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  42682  6  7 13:11 NAICS_data_2959.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  23749  6  7 13:11 NAICS_data_2988.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  32128  6  7 13:11 NAICS_data_2993.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  35820  6  7 13:11 NAICS_data_2994.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  34235  6  7 13:11 NAICS_data_3044.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  22993  6  7 13:11 NAICS_data_3073.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  36655  6  7 13:11 NAICS_data_3075.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44695  6  7 13:11 NAICS_data_3144.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  29347  6  7 13:11 NAICS_data_3197.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  18923  6  7 13:11 NAICS_data_3237.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  17069  6  7 13:11 NAICS_data_3494.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  38880  6  7 13:11 NAICS_data_3495.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  37050  6  7 13:11 NAICS_data_3731.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  29681  6  7 13:11 NAICS_data_4031.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  26408  6  7 13:11 NAICS_data_4125.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24377  6  7 13:11 NAICS_data_4213.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  29144  6  7 13:11 NAICS_data_4215.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  26366  6  7 13:11 NAICS_data_4329.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  43663  6  7 13:11 NAICS_data_4436.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  43874  6  7 13:11 NAICS_data_4525.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  31843  6  7 13:11 NAICS_data_4610.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  19093  6  7 13:11 NAICS_data_4618.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  39685  6  7 13:11 NAICS_data_4699.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  27853  6  7 13:11 NAICS_data_4896.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  23809  6  7 13:11 NAICS_data_4938.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  36561  6  7 13:11 NAICS_data_5060.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  30250  6  7 13:11 NAICS_data_5092.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  37161  6  7 13:11 NAICS_data_5305.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  32810  6  7 13:11 NAICS_data_5341.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44509  6  7 13:11 NAICS_data_5364.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24733  6  7 13:11 NAICS_data_5631.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  34316  6  7 13:11 NAICS_data_5890.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  40891  6  7 13:11 NAICS_data_5899.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  33036  6  7 13:11 NAICS_data_5992.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  35795  6  7 13:11 NAICS_data_6161.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  24555  6  7 13:11 NAICS_data_6181.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  45435  6  7 13:11 NAICS_data_6329.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  19628  6  7 13:11 NAICS_data_6335.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  25159  6  7 13:11 NAICS_data_6397.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  39471  6  7 13:11 NAICS_data_6493.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  37846  6  7 13:11 NAICS_data_6637.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  43998  6  7 13:11 NAICS_data_6700.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  17286  6  7 13:11 NAICS_data_6842.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  19536  6  7 13:11 NAICS_data_6904.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  28612  6  7 13:11 NAICS_data_7028.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  31113  6  7 13:11 NAICS_data_7102.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  18387  6  7 13:11 NAICS_data_7138.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  21292  6  7 13:11 NAICS_data_7226.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44306  6  7 13:11 NAICS_data_7338.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  21814  6  7 13:11 NAICS_data_7383.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  18958  6  7 13:11 NAICS_data_7388.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  27936  6  7 13:11 NAICS_data_7427.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  37189  6  7 13:11 NAICS_data_7535.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  35754  6  7 13:11 NAICS_data_7642.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  16856  6  7 13:11 NAICS_data_7677.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  16384  6  7 13:11 NAICS_data_7765.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  29368  6  7 13:11 NAICS_data_7830.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  18545  6  7 13:11 NAICS_data_7833.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  20289  6  7 13:11 NAICS_data_7845.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  42605  6  7 13:11 NAICS_data_7913.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44354  6  7 13:11 NAICS_data_8015.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  42409  6  7 13:11 NAICS_data_8085.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  42128  6  7 13:11 NAICS_data_8131.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  22841  6  7 13:11 NAICS_data_8196.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  25996  6  7 13:11 NAICS_data_8397.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  30444  6  7 13:11 NAICS_data_8403.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  23442  6  7 13:11 NAICS_data_8499.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  40761  6  7 13:11 NAICS_data_8522.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  25953  6  7 13:11 NAICS_data_8545.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  34603  6  7 13:11 NAICS_data_8700.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  29693  6  7 13:11 NAICS_data_8749.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  26090  6  7 13:11 NAICS_data_8760.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  44311  6  7 13:11 NAICS_data_8832.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  18877  6  7 13:11 NAICS_data_9012.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  22508  6  7 13:11 NAICS_data_9066.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  43654  6  7 13:11 NAICS_data_9103.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  30743  6  7 13:11 NAICS_data_9139.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  30066  6  7 13:11 NAICS_data_9165.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  38773  6  7 13:11 NAICS_data_9250.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  37443  6  7 13:11 NAICS_data_9448.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  21209  6  7 13:11 NAICS_data_9825.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  16605  6  7 13:11 NAICS_data_9834.csv\r\n",
      "-rw-r--r--  1 re4lfl0w  staff  45651  6  7 13:11 NAICS_data_9986.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l headerRemoved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAICS,NAICS Description,Item,Tax Status,Employer Status,2012 Revenue,2011 Revenue,2010 Revenue,2012 Coefficient of Variation,2011 Coefficient of Variation,2010 Coefficient of Variation\r",
      "\r\n",
      "561599,All Other Travel Arrangement and Reservation Services ,Commissions or fees from packaged tours ,All Establishments ,Employer Firms ,191,179,168,10.6,14.5,14\r",
      "\r\n",
      "51222,Integrated Record Production/Distribution ,Licensing revenue - Licensing of rights to use musical recordings ,All Establishments ,Employer Firms ,D,668,791,D,1,0.9\r",
      "\r\n",
      "56292,Material Recovery Facilities ,Total operating revenue ,All Establishments ,Employer Firms ,\"5,068\",\"5,842\",\"4,854\",5.3,5.1,5\r",
      "\r\n",
      "6239,Other Residential Care Facilities ,Patient out-of-pocket - Patients' assigned Social Security benefits ,All Establishments ,Employer Firms ,S,26,S,S,29.2,S\r",
      "\r\n",
      "62133,\"Offices of Mental Health Practitioners, (except Physicians) \",Total operating revenue ,All Establishments ,Employer Firms ,\"8,040\",\"7,350\",\"6,702\",4.5,4.3,4.4\r",
      "\r\n",
      "51112,Periodical Publishers ,Sale or licensing of rights to content ,All Establishments ,Employer Firms ,506,482,475,13.4,15.2,15.3\r",
      "\r\n",
      "56132,Temporary Help Services ,All other operating revenue ,All Establishments ,Employer Firms ,\"9,400\",\"8,736\",\"7,693\",6.1,7.8,7.8\r",
      "\r\n",
      "621498,All Other Outpatient Care Centers ,All other non-patient care revenue ,All Establishments ,Employer Firms ,\"2,010\",\"2,494\",\"2,375\",4.5,3.7,4.1\r",
      "\r\n",
      "6231,Nursing Care Facilities (Skilled Nursing Facilities)11 ,Patient out-of-pocket - Payments from patients and their families ,All Establishments ,Employer Firms ,\"11,659\",\"11,298\",\"11,247\",2.8,2.7,2.2\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head NAICS_data_1048.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561599,All Other Travel Arrangement and Reservation Services ,Commissions or fees from packaged tours ,All Establishments ,Employer Firms ,191,179,168,10.6,14.5,14\r",
      "\r\n",
      "51222,Integrated Record Production/Distribution ,Licensing revenue - Licensing of rights to use musical recordings ,All Establishments ,Employer Firms ,D,668,791,D,1,0.9\r",
      "\r\n",
      "56292,Material Recovery Facilities ,Total operating revenue ,All Establishments ,Employer Firms ,\"5,068\",\"5,842\",\"4,854\",5.3,5.1,5\r",
      "\r\n",
      "6239,Other Residential Care Facilities ,Patient out-of-pocket - Patients' assigned Social Security benefits ,All Establishments ,Employer Firms ,S,26,S,S,29.2,S\r",
      "\r\n",
      "62133,\"Offices of Mental Health Practitioners, (except Physicians) \",Total operating revenue ,All Establishments ,Employer Firms ,\"8,040\",\"7,350\",\"6,702\",4.5,4.3,4.4\r",
      "\r\n",
      "51112,Periodical Publishers ,Sale or licensing of rights to content ,All Establishments ,Employer Firms ,506,482,475,13.4,15.2,15.3\r",
      "\r\n",
      "56132,Temporary Help Services ,All other operating revenue ,All Establishments ,Employer Firms ,\"9,400\",\"8,736\",\"7,693\",6.1,7.8,7.8\r",
      "\r\n",
      "621498,All Other Outpatient Care Centers ,All other non-patient care revenue ,All Establishments ,Employer Firms ,\"2,010\",\"2,494\",\"2,375\",4.5,3.7,4.1\r",
      "\r\n",
      "6231,Nursing Care Facilities (Skilled Nursing Facilities)11 ,Patient out-of-pocket - Payments from patients and their families ,All Establishments ,Employer Firms ,\"11,659\",\"11,298\",\"11,247\",2.8,2.7,2.2\r",
      "\r\n",
      "51911,News Syndicates ,Total operating revenue ,All Establishments ,Employer Firms ,\"2,348\",\"2,188\",\"1,960\",4.5,3.2,3.1\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head headerRemoved/NAICS_data_1048.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/re4lfl0w/Documents/ipython/books/Automate_the_Boring_Sutff_with_Python\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제점\n",
    "\n",
    "- 무작정 삭제하니 첫번째 줄이 헤더줄인지 아닌지 알 수가 없네\n",
    "- 이걸 어떻게 구분해서 헤더가 있는 csv 파일만 헤더를 삭제하고 헤더가 없는건 그냥 놔두지?\n",
    "- 헤더가 없는 csv 파일을 건드려서 원본 data가 손상됐잖아."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam,eggs,bacon,ham\r",
      "\r\n",
      "\"Hello, world!\",eggs,bacon,ham\r",
      "\r\n",
      "1,2,3,3.141592,4\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello, world!\",eggs,bacon,ham\r",
      "\r\n",
      "1,2,3,3.141592,4\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat headerRemoved/output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read in the CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write Out the CSV file Without First Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vice versa: 거꾸로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Similar Programs\n",
    "\n",
    "- Compare data between different rows in a CSV file or between multiple CSV files.\n",
    "- Copy specific data from a CSV file to an Excel file, or vice versa.\n",
    "- Check for invalid data or formatting mistakes in CSV files and alert the user to these errors.\n",
    "- Read data from a CSV file as input for your Python programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API advantages\n",
    "\n",
    "- Scrape raw data from Web sites(Accessing APIs is often more convenient than downloading web pages and parsing HTML with Beautiful Soup.)\n",
    "- Automatically download new posts from one of your social network accounts and post them to another account. For example, you could take your Tumblr posts and post them to Facebook.\n",
    "- Create a \"movie encyclopedia\" for your personal movie collection by pulling data from IMDb, Rotten Tomatoes, and Wikipedia and putting it into a single text file on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The JSON Module\n",
    "\n",
    "- json.loads()\n",
    "- json.dumps()\n",
    "- JSON은 모든 파이썬 값들을 저장할 수 없다.\n",
    "- 오직 strings, integers, floats, Booleans, lists, dictionaries, and NoneType만 가능하다.\n",
    "- File objects, CSV Reader or Writer objects, Regex objects, or Selenium WebElement objects는 불가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.loads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function loads in module json:\n",
      "\n",
      "loads(s, encoding=None, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)\n",
      "    Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\n",
      "    document) to a Python object.\n",
      "    \n",
      "    If ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\n",
      "    other than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\n",
      "    must be specified. Encodings that are not ASCII based (such as UCS-2)\n",
      "    are not allowed and should be decoded to ``unicode`` first.\n",
      "    \n",
      "    ``object_hook`` is an optional function that will be called with the\n",
      "    result of any object literal decode (a ``dict``). The return value of\n",
      "    ``object_hook`` will be used instead of the ``dict``. This feature\n",
      "    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n",
      "    \n",
      "    ``object_pairs_hook`` is an optional function that will be called with the\n",
      "    result of any object literal decoded with an ordered list of pairs.  The\n",
      "    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n",
      "    This feature can be used to implement custom decoders that rely on the\n",
      "    order that the key and value pairs are decoded (for example,\n",
      "    collections.OrderedDict will remember the order of insertion). If\n",
      "    ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority.\n",
      "    \n",
      "    ``parse_float``, if specified, will be called with the string\n",
      "    of every JSON float to be decoded. By default this is equivalent to\n",
      "    float(num_str). This can be used to use another datatype or parser\n",
      "    for JSON floats (e.g. decimal.Decimal).\n",
      "    \n",
      "    ``parse_int``, if specified, will be called with the string\n",
      "    of every JSON int to be decoded. By default this is equivalent to\n",
      "    int(num_str). This can be used to use another datatype or parser\n",
      "    for JSON integers (e.g. float).\n",
      "    \n",
      "    ``parse_constant``, if specified, will be called with one of the\n",
      "    following strings: -Infinity, Infinity, NaN, null, true, false.\n",
      "    This can be used to raise an exception if invalid JSON numbers\n",
      "    are encountered.\n",
      "    \n",
      "    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n",
      "    kwarg; otherwise ``JSONDecoder`` is used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dumps in module json:\n",
      "\n",
      "dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, sort_keys=False, **kw)\n",
      "    Serialize ``obj`` to a JSON formatted ``str``.\n",
      "    \n",
      "    If ``skipkeys`` is false then ``dict`` keys that are not basic types\n",
      "    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``)\n",
      "    will be skipped instead of raising a ``TypeError``.\n",
      "    \n",
      "    If ``ensure_ascii`` is false, all non-ASCII characters are not escaped, and\n",
      "    the return value may be a ``unicode`` instance. See ``dump`` for details.\n",
      "    \n",
      "    If ``check_circular`` is false, then the circular reference check\n",
      "    for container types will be skipped and a circular reference will\n",
      "    result in an ``OverflowError`` (or worse).\n",
      "    \n",
      "    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n",
      "    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n",
      "    strict compliance of the JSON specification, instead of using the\n",
      "    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n",
      "    \n",
      "    If ``indent`` is a non-negative integer, then JSON array elements and\n",
      "    object members will be pretty-printed with that indent level. An indent\n",
      "    level of 0 will only insert newlines. ``None`` is the most compact\n",
      "    representation.  Since the default item separator is ``', '``,  the\n",
      "    output might include trailing whitespace when ``indent`` is specified.\n",
      "    You can use ``separators=(',', ': ')`` to avoid this.\n",
      "    \n",
      "    If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n",
      "    then it will be used instead of the default ``(', ', ': ')`` separators.\n",
      "    ``(',', ':')`` is the most compact JSON representation.\n",
      "    \n",
      "    ``encoding`` is the character encoding for str instances, default is UTF-8.\n",
      "    \n",
      "    ``default(obj)`` is a function that should return a serializable version\n",
      "    of obj or raise TypeError. The default simply raises TypeError.\n",
      "    \n",
      "    If *sort_keys* is ``True`` (default: ``False``), then the output of\n",
      "    dictionaries will be sorted by key.\n",
      "    \n",
      "    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n",
      "    ``.default()`` method to serialize additional types), specify it with\n",
      "    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(json.dumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON with the loads() Function\n",
    "\n",
    "- loads(load string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringOfJsonData = '{\"name\": \"Zophie\", \"isCat\": true, \"miceCaught\": 0, \"felineIQ\": null}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stringOfJsonData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonDataAsPythonValue = json.loads(stringOfJsonData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'felineIQ': None, u'isCat': True, u'miceCaught': 0, u'name': u'Zophie'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonDataAsPythonValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jsonDataAsPythonValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON strings은 항상 double quotes를 사용한다.\n",
    "- 돌려주는 것은 파이썬 사전과 같다.\n",
    "- 딕셔너리는 순서가 없고, key-value pairs다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing JSON with the dumps() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pythonValue = {'isCat': True, 'miceCaught': 0, 'name': 'Zophie', 'felineIQ': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringOfJsonData = json.dumps(pythonValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"miceCaught\": 0, \"isCat\": true, \"felineIQ\": null, \"name\": \"Zophie\"}'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringOfJsonData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON에 포함될 수 있는 데이터 타입\n",
    "  - dictionary\n",
    "  - list\n",
    "  - integer\n",
    "  - float\n",
    "  - string\n",
    "  - Boolean\n",
    "  - None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Fetching Current Weather Data\n",
    "\n",
    "### High Level Logic\n",
    "\n",
    "- Reads the requested location from the command line.\n",
    "- Downloads JSON weather data from OpenWeatherMap.org\n",
    "- Converts the string of JSON data to a Python data structure.\n",
    "- Print the weather for today and the next two days.\n",
    "\n",
    "### Code Level Logic\n",
    "\n",
    "- Join strings in sys.argv to get the location\n",
    "- Call requests.get() to download the weather data.\n",
    "- Call json.loads() to convert the JSON data to a Python data structure.\n",
    "- Print the weather forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Location from the Command Line Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import optparse\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = optparse.OptionParser(usage='%prog -l <location>', version='0.1')\n",
    "    parser.add_option('-l',\n",
    "                      dest='location',\n",
    "                      type='string',\n",
    "                      help='To insert location')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    \n",
    "    # TODO: Download the JSON data from OpenWeatherMap.org's API.\n",
    "    \n",
    "    # TODO: Load JSOn data into a Python variable.\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optparse 연습\n",
    "\n",
    "- [optparse – Command line option parser to replace getopt. \\- Python Module of the Week](http://pymotw.com/2/optparse/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Values at 0x110458998: {'a': False, 'c': 3, 'b': 'val'}>, [])\n"
     ]
    }
   ],
   "source": [
    "import optparse\n",
    "\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('-a', action=\"store_true\", default=False)\n",
    "parser.add_option('-b', action=\"store\", dest=\"b\")\n",
    "parser.add_option('-c', action=\"store\", dest=\"c\", type=\"int\")\n",
    "\n",
    "print parser.parse_args(['-a', '-bval', '-c', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Values at 0x110458f38: {'a': False, 'c': 3, 'b': 'val'}>, [])\n"
     ]
    }
   ],
   "source": [
    "# -a를 설정하지 않으면 기본값인 False로 세팅되는구나. \n",
    "# dest는 만들지도 않았는데 어떻게 저장되지?\n",
    "import optparse\n",
    "\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('-a', action=\"store_true\", default=False)\n",
    "parser.add_option('-b', action=\"store\", dest=\"b\")\n",
    "parser.add_option('-c', action=\"store\", dest=\"c\", type=\"int\")\n",
    "\n",
    "print parser.parse_args(['-bval', '-c', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Values at 0x11045f5f0: {'a': True, 'c': 3, 'b': 'val'}>, [])\n"
     ]
    }
   ],
   "source": [
    "import optparse\n",
    "\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('-a', action=\"store_true\", default=False)\n",
    "parser.add_option('-b', action=\"store\")\n",
    "parser.add_option('-c', action=\"store\", dest=\"c\", type=\"int\")\n",
    "\n",
    "print parser.parse_args(['-a', '-bval', '-c', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_callback:\n",
      "\toption: <Option at 0x110458ef0: --with>\n",
      "\topt_str: --with\n",
      "\tvalue: foo\n",
      "\tparser: <optparse.OptionParser instance at 0x1104641b8>\n",
      "flag_callback:\n",
      "\toption: <Option at 0x110458d40: --flag>\n",
      "\topt_str: --flag\n",
      "\tvalue: None\n",
      "\tparser: <optparse.OptionParser instance at 0x1104641b8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Values at 0x110458e60: {'with': None}>, [])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optparse\n",
    "\n",
    "def flag_callback(option, opt_str, value, parser):\n",
    "    print 'flag_callback:'\n",
    "    print '\\toption:', repr(option)\n",
    "    print '\\topt_str:', opt_str\n",
    "    print '\\tvalue:', value\n",
    "    print '\\tparser:', parser\n",
    "    return\n",
    "\n",
    "def with_callback(option, opt_str, value, parser):\n",
    "    print 'with_callback:'\n",
    "    print '\\toption:', repr(option)\n",
    "    print '\\topt_str:', opt_str\n",
    "    print '\\tvalue:', value\n",
    "    print '\\tparser:', parser\n",
    "    return\n",
    "\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('--flag', action=\"callback\", callback=flag_callback)\n",
    "parser.add_option('--with', \n",
    "                  action=\"callback\",\n",
    "                  callback=with_callback,\n",
    "                  type=\"string\",\n",
    "                  help=\"Include optional feature\")\n",
    "\n",
    "parser.parse_args(['--with', 'foo', '--flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_callback:\n",
      "\toption: <Option at 0x11045fd40: --with>\n",
      "\topt_str: --with\n",
      "\tvalue: ('foo', 'bar')\n",
      "\tparser: <optparse.OptionParser instance at 0x11045f638>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Values at 0x11045fab8: {'with': None}>, [])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optparse\n",
    "\n",
    "def with_callback(option, opt_str, value, parser):\n",
    "    print 'with_callback:'\n",
    "    print '\\toption:', repr(option)\n",
    "    print '\\topt_str:', opt_str\n",
    "    print '\\tvalue:', value\n",
    "    print '\\tparser:', parser\n",
    "    return\n",
    "\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('--with', \n",
    "                  action=\"callback\",\n",
    "                  callback=with_callback,\n",
    "                  type=\"string\",\n",
    "                  nargs=2,\n",
    "                  help=\"Include optional feature\")\n",
    "\n",
    "parser.parse_args(['--with', 'foo', 'bar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting quickWeather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quickWeather.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import optparse\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = optparse.OptionParser(usage='%prog -l <location>', version='0.1')\n",
    "    parser.add_option('-l',\n",
    "                      dest='location',\n",
    "                      type='string',\n",
    "                      help='To insert location')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    \n",
    "    # TODO: Download the JSON data from OpenWeatherMap.org's API.\n",
    "    url = 'http://api.openweathermap.org/data/2.5/forecast/daily?q={}&cnt=3'.format(options.location)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    pprint(response.text)\n",
    "    \n",
    "    \n",
    "    # TODO: Load JSOn data into a Python variable.\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'{\"cod\":\"200\",\"message\":0.0391,\"city\":{\"id\":1835848,\"name\":\"Seoul\",\"coord\":{\"lon\":126.977829,\"lat\":37.56826},\"country\":\"KR\",\"population\":0,\"sys\":{\"population\":0}},\"cnt\":3,\"list\":[{\"dt\":1433646000,\"temp\":{\"day\":298.4,\"min\":287.64,\"max\":299.52,\"night\":287.64,\"eve\":298.1,\"morn\":298.4},\"pressure\":1005.13,\"humidity\":61,\"weather\":[{\"id\":800,\"main\":\"Clear\",\"description\":\"sky is clear\",\"icon\":\"01d\"}],\"speed\":2.37,\"deg\":211,\"clouds\":0},{\"dt\":1433732400,\"temp\":{\"day\":296.6,\"min\":286.28,\"max\":296.86,\"night\":289.85,\"eve\":295.81,\"morn\":286.28},\"pressure\":999.19,\"humidity\":64,\"weather\":[{\"id\":803,\"main\":\"Clouds\",\"description\":\"broken clouds\",\"icon\":\"04d\"}],\"speed\":1.74,\"deg\":274,\"clouds\":56},{\"dt\":1433818800,\"temp\":{\"day\":297.38,\"min\":285.79,\"max\":298.95,\"night\":286.28,\"eve\":297.91,\"morn\":285.79},\"pressure\":997.59,\"humidity\":64,\"weather\":[{\"id\":800,\"main\":\"Clear\",\"description\":\"sky is clear\",\"icon\":\"02d\"}],\"speed\":1.81,\"deg\":264,\"clouds\":8}]}\\n'\n"
     ]
    }
   ],
   "source": [
    "%run quickWeather.py -l Seoul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load JSON Data and Print Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile quickWeather.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import optparse\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = optparse.OptionParser(usage='%prog -l <location>', version='0.1')\n",
    "    parser.add_option('-l',\n",
    "                      dest='location',\n",
    "                      type='string',\n",
    "                      help='To insert location')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    \n",
    "    # TODO: Download the JSON data from OpenWeatherMap.org's API.\n",
    "    url = 'http://api.openweathermap.org/data/2.5/forecast/daily?q={}&cnt=3'.format(options.location)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # TODO: Load JSON data into a Python variable.\n",
    "    weatherData = json.loads(response.text)\n",
    "    pprint(weatherData)\n",
    "    print('')\n",
    "    \n",
    "    # Print weather descriptions.\n",
    "    w = weatherData['list']\n",
    "    print('Current weather in {}:'.format(options.location))\n",
    "    print('{} - {}'.format(w[0]['weather'][0]['main'], \n",
    "                         w[0]['weather'][0]['description']))\n",
    "    print('')\n",
    "    print('Tomorrow:')\n",
    "    print('{} - {}'.format(w[1]['weather'][0]['main'], \n",
    "                          w[1]['weather'][0]['description']))\n",
    "    print('')\n",
    "    print('Day after tomorrow:')\n",
    "    print('{} - {}'.format(w[2]['weather'][0]['main'],\n",
    "                   w[2]['weather'][0]['description']))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting quickWeather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quickWeather.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import optparse\n",
    "from pprint import pprint\n",
    "\n",
    "def print_weather(w, i):\n",
    "    print('{} - {}'.format(w[i]['weather'][0]['main'], \n",
    "                         w[i]['weather'][0]['description']))\n",
    "    print('')\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = optparse.OptionParser(usage='%prog -l <location>', version='0.1')\n",
    "    parser.add_option('-l',\n",
    "                      dest='location',\n",
    "                      type='string',\n",
    "                      help='To insert location')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    \n",
    "    # TODO: Download the JSON data from OpenWeatherMap.org's API.\n",
    "    url = 'http://api.openweathermap.org/data/2.5/forecast/daily?q={}&cnt=3'.format(options.location)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # TODO: Load JSON data into a Python variable.\n",
    "    weatherData = json.loads(response.text)\n",
    "    pprint(weatherData)\n",
    "    print('')\n",
    "    \n",
    "    # Print weather descriptions.\n",
    "    w = weatherData['list']\n",
    "    print('Current weather in {}:'.format(options.location))\n",
    "    \n",
    "    for i, item in enumerate(w):\n",
    "        if i == 0:\n",
    "            print_weather(w, i)\n",
    "        if i == 1:\n",
    "            print('Tomorrow:')\n",
    "            print_weather(w, i)\n",
    "        if i == 2:\n",
    "            print('Day after tomorrow:')\n",
    "            print_weather(w, i)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'city': {u'coord': {u'lat': 37.56826, u'lon': 126.977829},\n",
      "           u'country': u'KR',\n",
      "           u'id': 1835848,\n",
      "           u'name': u'Seoul',\n",
      "           u'population': 0,\n",
      "           u'sys': {u'population': 0}},\n",
      " u'cnt': 3,\n",
      " u'cod': u'200',\n",
      " u'list': [{u'clouds': 0,\n",
      "            u'deg': 211,\n",
      "            u'dt': 1433646000,\n",
      "            u'humidity': 61,\n",
      "            u'pressure': 1005.13,\n",
      "            u'speed': 2.37,\n",
      "            u'temp': {u'day': 298.4,\n",
      "                      u'eve': 298.1,\n",
      "                      u'max': 299.52,\n",
      "                      u'min': 287.64,\n",
      "                      u'morn': 298.4,\n",
      "                      u'night': 287.64},\n",
      "            u'weather': [{u'description': u'sky is clear',\n",
      "                          u'icon': u'01d',\n",
      "                          u'id': 800,\n",
      "                          u'main': u'Clear'}]},\n",
      "           {u'clouds': 56,\n",
      "            u'deg': 274,\n",
      "            u'dt': 1433732400,\n",
      "            u'humidity': 64,\n",
      "            u'pressure': 999.19,\n",
      "            u'speed': 1.74,\n",
      "            u'temp': {u'day': 296.6,\n",
      "                      u'eve': 295.81,\n",
      "                      u'max': 296.86,\n",
      "                      u'min': 286.28,\n",
      "                      u'morn': 286.28,\n",
      "                      u'night': 289.85},\n",
      "            u'weather': [{u'description': u'broken clouds',\n",
      "                          u'icon': u'04d',\n",
      "                          u'id': 803,\n",
      "                          u'main': u'Clouds'}]},\n",
      "           {u'clouds': 8,\n",
      "            u'deg': 264,\n",
      "            u'dt': 1433818800,\n",
      "            u'humidity': 64,\n",
      "            u'pressure': 997.59,\n",
      "            u'speed': 1.81,\n",
      "            u'temp': {u'day': 297.38,\n",
      "                      u'eve': 297.91,\n",
      "                      u'max': 298.95,\n",
      "                      u'min': 285.79,\n",
      "                      u'morn': 285.79,\n",
      "                      u'night': 286.28},\n",
      "            u'weather': [{u'description': u'sky is clear',\n",
      "                          u'icon': u'02d',\n",
      "                          u'id': 800,\n",
      "                          u'main': u'Clear'}]}],\n",
      " u'message': 0.0391}\n",
      "\n",
      "Current weather in Seoul:\n",
      "Clear - sky is clear\n",
      "\n",
      "Tomorrow:\n",
      "Clouds - broken clouds\n",
      "\n",
      "Day after tomorrow:\n",
      "Clear - sky is clear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run quickWeather.py -l Seoul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Similar Programs\n",
    "\n",
    "- Collect weather forecasts for several campsites or hiking trails to see which one will have the best weather.\n",
    "- Schedule a program to regularly check the weather and send you a frost alert if you need to move your plaints indoors. (Chapter 15 covers scheduling, and Chapter 16 explains how to send email.)\n",
    "- Pull weather data from multiple sites to show all at aonce, or calculate and show the average of the multiple weather predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- CSV and JSON는 데이터 저장하는 plaintext다. 인간이 읽기 쉽다.\n",
    "- 간단한 스프레드시트나 웹 앱 데이터에 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pratice Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel-to-CSV Converter\n",
    "\n",
    "- 엑셀은 여러개의 시트를 포함하고 있다.\n",
    "- 1개의 시트당 1개의 csv 파일을 만든다.\n",
    "        <excel filename>_<sheet title>.csv 로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 개 파일 동시에 열기\n",
    "\n",
    "- [How can I open multiple files using \"with open\" in Python? \\- Stack Overflow](http://stackoverflow.com/questions/4617034/how-can-i-open-multiple-files-using-with-open-in-python)\n",
    "```python\n",
    "with open('a', 'w') as a, open('b', 'w') as b:\n",
    "    do_something()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전체적인 큰 그림\n",
    "\n",
    "for excelFile in os.listdir('.'):\n",
    "    # Skip non-xlsx files, load the workbook object.\n",
    "    for sheetName in wb.get_sheet_names():\n",
    "        # Loop through every sheet in the workbook.\n",
    "        sheet = wb.get_sheet_by_name(sheetName)\n",
    "\n",
    "        # Create the CSV filename from the Excel filename and sheet title.\n",
    "        # Create the csv.writer object for this CSV file.\n",
    "\n",
    "        # Loop through every row in the sheet.\n",
    "        for rowNum in range(1, sheet.get_highest_row() + 1):\n",
    "            rowData = []    # append each cell to this list\n",
    "            # Loop through each cell in the row.\n",
    "            for colNum in range(1, sheet.get_highest_column() + 1):\n",
    "                # Append each cell's data to rowData.\n",
    "\n",
    "            # Write the rowData list to the CSV file.\n",
    "\n",
    "        csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
