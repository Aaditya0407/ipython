{
 "metadata": {
  "name": "",
  "signature": "sha256:86149b01ca9e941b5f91df4317155e6ea099b24c96821072d6e65cd344eae6a4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# \uc6f9\uc5d0\uc11c \uc790\ub3d9\uc73c\ub85c E-book \ubc1b\uae30\n",
      "\n",
      "- [Ebook \uad49\uc7a5\ud788 \ub9ce\ub2e4!](http://ebooks.shahed.biz/)\n",
      "- \uc74c.. \ub0b4\uac00 \ud588\uc744\ub54c\ub294 \uc798 \ub410\ub294\ub370 \uc790\ub3d9\ub2e4\uc6b4\ub85c\ub4dc \ud558\ub294 \uc0ac\ub78c\ub4e4\uc774 \ub9ce\uc544\uc11c \uadf8\ub7f0\uac00 **\ub9c9\uc544\ub1a8\ub124** -\u3141-(2014.07 \uc815\ub3c4)\n",
      "- \uc9c0\uae08\uc740 id/pw \uc785\ub825\ud574\uc57c \ub41c\ub2e4. \uadf8\ub798\uc11c \uc9c0\uae08 \uc774 \uc18c\uc2a4\ub97c \ub3cc\ub9ac\uba74 \ubc14\ub85c \uc885\ub8cc\ub428..\n",
      "- id/pw \uc790\ub3d9 \uc785\ub825\ud574\uc11c \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc744 \uc218 \uc788\uac8c \uc5c5\uadf8\ub808\uc774\ub4dc \ud574\uc57c \ub428. \uadfc\ub370 \uadc0\ucc2e\uc74c. \uadf8\uac74 **\ub3c5\uc790\uc758 \ubaab**\uc73c\ub85c \ub0a8\uae40\n",
      "\n",
      "![auto download](files/images/auto_download.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## \ub9cc\ub4e4\uac8c \ub41c \uacc4\uae30\n",
      "\n",
      "- \ud558\ub098\uc529 \ud074\ub9ad\ud558\uba74\uc11c \ub2e4\uc6b4 \ubc1b\ub294\uac74 \ub178\uac00\ub2e4\ub2c8\uae4c..\n",
      "- \ud3f4\ub354\ub3c4 \uc790\ub3d9\uc73c\ub85c \uc0dd\uc131\n",
      "- \uc7ac\uadc0\uc801\uc73c\ub85c \ud3f4\ub354, \ud30c\uc77c \ubd84\ub958\n",
      "\n",
      "## \ub2e4\uc591\ud55c \ubc29\ubc95 \uac00\ub2a5\n",
      "\n",
      "1. \uc190\uc73c\ub85c..\n",
      "2. \ud234\uc758 \ub3c4\uc6c0(Download master \uac19\uc740)\n",
      "3. [Download master - Chrome Extension](https://chrome.google.com/webstore/detail/download-master/mcceagdollnkjlogmdckgjakjapmkdjf): \uc774\uac83\ub3c4 \uc88b\ub2e4. \ub9c1\ud06c\ub4e4\ub9cc \ucd94\ucd9c\ud574\uc8fc\uace0 \ub2e4\uc6b4\ubc1b\uc744 \uc218 \uc788\uac8c \ub3c4\uc640\uc900\ub2e4. \uadf8\ub7f0\ub370 \uc11c\ube0c \ub514\ub809\ud1a0\ub9ac\uae4c\uc9c0 \uc788\ub294\uac74 \uc57d\uac04\uc758 \ub178\uac00\ub2e4\uac00 \ud544\uc694\ud558\ub2e4. \ucd5c\uc885\uacb0\ub860: \uc774\uac74 \ubc18\ub178\uac00\ub2e4\n",
      "4. \uc9c1\uc811 \uc9dc\ubcf8\ub2e4. **\uc2a4\ud06c\ub9bd\ud2b8 \uc791\uc131**(\uc774\uac78\ub85c \uc120\ud0dd)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\ub294 Logic\n",
      "\n",
      "1. Base_url\uc744 \uc785\ub825\ud55c\ub2e4.\n",
      "2. requests\ub97c \ud574\uc11c reponse\ub97c \ubc1b\uc544\uc628\ub2e4.\n",
      "3. get_links\uc5d0 response\ub97c \ub118\uaca8\uc11c links\ub97c \ucd94\ucd9c\ud55c\ub2e4.(\uc0c1\ub300\uacbd\ub85c \ud638\ud658\ud574\uc918\uc57c \ub41c\ub2e4.)\n",
      "4. \ub514\ub809\ud1a0\ub9ac\ub77c\uba74 \ud55c \ubc88 \ub354 \ub4e4\uc5b4\uac00\uace0 \ud30c\uc77c\uc774\uba74 \uadf8 \ud30c\uc77c\uc744 \ubc1b\uc544\uc628\ub2e4.\n",
      "5. \ud30c\uc77c\uc744 \uc800\uc7a5\ud560 \ub54c \ub514\ub809\ud1a0\ub9ac\uac00 \uc5c6\uc73c\uba74 \uc0dd\uc131\ud574\uc900\ub2e4.\n",
      "6. \ud30c\uc77c\uc744 \uc800\uc7a5\ud55c\ub2e4."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## \ucd5c\uc885 \uc18c\uc2a4 \ucf54\ub4dc "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8\n",
      "\n",
      "import requests\n",
      "import re\n",
      "import os\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def is_dir(string):\n",
      "    \"\"\"confirm ended words with /\"\"\"\n",
      "    if re.search(r'.*?\\/$', string):\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "def is_file(string):\n",
      "    \"\"\"confirm ended words without /\"\"\"\n",
      "    if re.search(r'[^\\/]$', string):\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "def replace_os_character(string):\n",
      "    \"\"\"misunderstand chracters in OS change to '_'.\"\"\"\n",
      "    # except /\n",
      "    string = re.sub(r'\\\\|\\?|\\:|\\*|\\\"|\\>|\\<|\\|', '_', string)\n",
      "    return string\n",
      "\n",
      "def get_links(soup, base_url):\n",
      "    \"\"\"get links\"\"\"\n",
      "    links = []\n",
      "    for link in soup.find_all('a'):\n",
      "        link = link.get('href').strip()\n",
      "        if re.search(r'^[A-Za-z0-9]', link) and \\\n",
      "                not 'www.shahed.biz' in link:\n",
      "            links.append(os.path.join(base_url, link))\n",
      "    return links\n",
      "\n",
      "def do_file_work(link, save_base_path='/tmp'):\n",
      "    \"\"\"file download\"\"\"\n",
      "    try:\n",
      "        print 'downloading...', link\n",
      "        response = requests.get(link)\n",
      "        filename = re.sub(r'http://ebooks.shahed.biz/', '', link)\n",
      "        fullpath = os.path.join(save_base_path, filename)\n",
      "        fullpath = urllib2.unquote(fullpath)\n",
      "        fullpath = replace_os_character(fullpath)\n",
      "        path, filename = os.path.split(fullpath)\n",
      "        write_file(fullpath, path, filename, response)\n",
      "    except Exception, e:\n",
      "        print e\n",
      "\n",
      "def write_file(fullpath, path, filename, response):\n",
      "    \"\"\"make dir and write file\"\"\"\n",
      "    if not os.path.exists(path):\n",
      "        os.makedirs(path)\n",
      "\n",
      "    try:\n",
      "        with open(fullpath, 'wb') as f:\n",
      "            f.write(response.content)\n",
      "            print '[+] wrote', filename\n",
      "    except Exception, e:\n",
      "        print e\n",
      "\n",
      "def search(url):\n",
      "    \"\"\"main module\"\"\"\n",
      "    try:\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content)\n",
      "        links = get_links(soup, url)\n",
      "        for link in links:\n",
      "            # link = os.path.join(url, link)\n",
      "            if is_dir(link):\n",
      "                print '+', link\n",
      "                search(link)\n",
      "            else:\n",
      "                print '-', link\n",
      "                do_file_work(link)\n",
      "            print ''\n",
      "    except Exception as e:\n",
      "        print e\n",
      "\n",
      "base_url = 'http://ebooks.shahed.biz/'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    search(base_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}