{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹에서 자동으로 E-book 받기\n",
    "\n",
    "- [Ebook 굉장히 많다!](http://ebooks.shahed.biz/)\n",
    "- 음.. 내가 했을때는 잘 됐는데 자동다운로드 하는 사람들이 많아서 그런가 **막아놨네** -ㅁ-(2014.07 정도)\n",
    "- 지금은 id/pw 입력해야 된다. 그래서 지금 이 소스를 돌리면 바로 종료됨..\n",
    "- id/pw 자동 입력해서 다운로드 받을 수 있게 업그레이드 해야 됨. 근데 귀찮음. 그건 **독자의 몫**으로 남김\n",
    "\n",
    "![ebooks](files/images/auto_download_site.png)\n",
    "\n",
    "<img src=\"files/images/auto_download.png\" width=600px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 만들게 된 계기\n",
    "\n",
    "- 하나씩 클릭하면서 다운 받는건 노가다니까..\n",
    "- 폴더도 자동으로 생성\n",
    "- 재귀적으로 폴더, 파일 분류\n",
    "\n",
    "## 다양한 방법 가능\n",
    "\n",
    "1. 손으로..\n",
    "2. 툴의 도움(Download master 같은)\n",
    "3. [Download master - Chrome Extension](https://chrome.google.com/webstore/detail/download-master/mcceagdollnkjlogmdckgjakjapmkdjf): 이것도 좋다. 링크들만 추출해주고 다운받을 수 있게 도와준다. 그런데 서브 디렉토리까지 있는건 약간의 노가다가 필요하다. 최종결론: 이건 반노가다\n",
    "4. 직접 짜본다. **스크립트 작성**(이걸로 선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다운로드 받는 Logic\n",
    "\n",
    "1. Base_url을 입력한다.\n",
    "2. requests를 해서 reponse를 받아온다.\n",
    "3. get_links에 response를 넘겨서 links를 추출한다.(상대경로 호환해줘야 된다.)\n",
    "4. 디렉토리라면 한 번 더 들어가고 파일이면 그 파일을 받아온다.\n",
    "5. 파일을 저장할 때 디렉토리가 없으면 생성해준다.\n",
    "6. 파일을 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 소스 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def is_dir(string):\n",
    "    \"\"\"confirm ended words with /\"\"\"\n",
    "    if re.search(r'.*?\\/$', string):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_file(string):\n",
    "    \"\"\"confirm ended words without /\"\"\"\n",
    "    if re.search(r'[^\\/]$', string):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def replace_os_character(string):\n",
    "    \"\"\"misunderstand chracters in OS change to '_'.\"\"\"\n",
    "    # except /\n",
    "    string = re.sub(r'\\\\|\\?|\\:|\\*|\\\"|\\>|\\<|\\|', '_', string)\n",
    "    return string\n",
    "\n",
    "def get_links(soup, base_url):\n",
    "    \"\"\"get links\"\"\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        link = link.get('href').strip()\n",
    "        if re.search(r'^[A-Za-z0-9]', link) and \\\n",
    "                not 'www.shahed.biz' in link:\n",
    "            links.append(os.path.join(base_url, link))\n",
    "    return links\n",
    "\n",
    "def do_file_work(link, save_base_path='/tmp'):\n",
    "    \"\"\"file download\"\"\"\n",
    "    try:\n",
    "        print 'downloading...', link\n",
    "        response = requests.get(link)\n",
    "        filename = re.sub(r'http://ebooks.shahed.biz/', '', link)\n",
    "        fullpath = os.path.join(save_base_path, filename)\n",
    "        fullpath = urllib2.unquote(fullpath)\n",
    "        fullpath = replace_os_character(fullpath)\n",
    "        path, filename = os.path.split(fullpath)\n",
    "        write_file(fullpath, path, filename, response)\n",
    "    except Exception, e:\n",
    "        print e\n",
    "\n",
    "def write_file(fullpath, path, filename, response):\n",
    "    \"\"\"make dir and write file\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    try:\n",
    "        with open(fullpath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            print '[+] wrote', filename\n",
    "    except Exception, e:\n",
    "        print e\n",
    "\n",
    "def search(url):\n",
    "    \"\"\"main module\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        links = get_links(soup, url)\n",
    "        for link in links:\n",
    "            # link = os.path.join(url, link)\n",
    "            if is_dir(link):\n",
    "                print '+', link\n",
    "                search(link)\n",
    "            else:\n",
    "                print '-', link\n",
    "                do_file_work(link)\n",
    "            print ''\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "base_url = 'http://ebooks.shahed.biz/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search(base_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
